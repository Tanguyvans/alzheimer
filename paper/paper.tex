\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Multi-Cohort Alzheimer's Classification via MRI-Clinical Fusion}

\author{\IEEEauthorblockN{Tanguy Vansnick}
\IEEEauthorblockA{\textit{Faculty of Engineering - ILIA} \\
\textit{University of Mons}\\
Rue de Houdain 9, 7000 Mons, Belgium \\
tanguy.vansnick@umons.ac.be}
\and
\IEEEauthorblockN{Maxime Gloesener}
\IEEEauthorblockA{\textit{Faculty of Engineering - ILIA} \\
\textit{University of Mons}\\
Rue de Houdain 9, 7000 Mons, Belgium \\
maxime.gloesener@umons.ac.be}
\and
\IEEEauthorblockN{Otmane Amel}
\IEEEauthorblockA{\textit{Faculty of Engineering - ILIA} \\
\textit{University of Mons}\\
Rue de Houdain 9, 7000 Mons, Belgium \\
otmane.amel@umons.ac.be}
\and
\IEEEauthorblockN{Vito Tota}
\IEEEauthorblockA{\textit{Faculty of Engineering - ILIA} \\
\textit{University of Mons}\\
Rue de Houdain 9, 7000 Mons, Belgium \\
vito.tota@umons.ac.be}
\and
\IEEEauthorblockN{Mathis Delehouzee}
\IEEEauthorblockA{\textit{Faculty of Engineering - ILIA} \\
\textit{University of Mons}\\
Rue de Houdain 9, 7000 Mons, Belgium \\
mathis.delehouzee@umons.ac.be}
\and
\IEEEauthorblockN{Sa\"id Mahmoudi}
\IEEEauthorblockA{\textit{Faculty of Engineering - ILIA} \\
\textit{University of Mons}\\
Rue de Houdain 9, 7000 Mons, Belgium \\
said.mahmoudi@umons.ac.be}
\and
\IEEEauthorblockN{for the Alzheimer's Disease}
\IEEEauthorblockN{Neuroimaging Initiative*}
}

\maketitle

\begin{abstract}
Deep learning for Alzheimer's disease (AD) classification from MRI has shown promise, yet recent reviews reveal that fewer than 20\% of studies employ external validation, with data leakage (e.g., same subjects in train and test sets) inflating reported accuracies to 95-99\%. Studies with rigorous subject-level splitting achieve 66-90\%. We present an end-to-end multimodal framework combining Vision Transformer (ViT) with masked autoencoder pretraining for 3D MRI analysis and Feature Tokenizer Transformer (FT-Transformer) for clinical tabular data, integrated through bidirectional cross-attention. We validate on 6,065 subjects from three independent cohorts (ADNI, OASIS, NACC) using strict subject-level 5-fold cross-validation. Our approach achieves \textbf{92.4\% accuracy} (AUC: 0.96) for distinguishing cognitively normal subjects from AD-trajectory patients (including MCI patients who later developed AD), and \textbf{93.3\%} on established AD cases.
\end{abstract}

\begin{IEEEkeywords}
Alzheimer's disease, multimodal fusion, Vision Transformer, deep learning, MRI classification
\end{IEEEkeywords}

\section{Introduction}

Alzheimer's disease (AD) is the most common cause of dementia, contributing to 60--70\% of the 57 million dementia cases worldwide \cite{who2023}. Early and accurate diagnosis is crucial for patient care, clinical trials enrollment, and treatment planning. Early diagnosis increasingly relies on biomarkers---CSF analysis requiring lumbar puncture, or amyloid PET imaging with radioactive tracers---which are impractical for routine screening. Structural MRI combined with clinical tabular data offers a practical alternative, capturing characteristic patterns of brain atrophy alongside demographic and cognitive assessments without invasive procedures or radiation exposure.

Deep learning approaches for AD classification have proliferated in recent years, with many studies reporting accuracies exceeding 95\%. However, a recent scoping review by Young et al. \cite{young2025} revealed that fewer than 20\% of published studies employ external validation, and methodological issues remain widespread. The primary issue is data leakage: when data from the same subject appears in both training and test sets, models learn to recognize individual brain morphology rather than disease-specific patterns.

Yagis et al. \cite{yagis2021} quantified this effect: when 2D slices from the same subject appear in both train and test sets, models achieve 96\% accuracy even with \textit{randomly assigned labels}---demonstrating that they learn to recognize individual brain morphology rather than disease patterns. With proper subject-level splitting, random-label accuracy drops to 50\% (as expected), and reported accuracies for real labels fall to 66-90\% \cite{young2025}.

Combining multiple data modalities has shown promise for improving AD classification. Clinical tabular data (demographics, cognitive assessments) provides complementary information to structural MRI. However, some clinical scores are strongly tied to cognitive status. The Mini-Mental State Examination (MMSE), a 30-point clinician-administered cognitive test, showed median scores of 29 for CN versus 23.5 for AD in the ADNI cohort \cite{qiu2020}---making it highly predictive of diagnostic group. We explore combining modern transformer architectures---specifically ViT for 3D MRI and FT-Transformer for tabular data---while excluding such cognitive scores to ensure the model learns from imaging and non-cognitive clinical features.

In this work, we present a fully end-to-end multimodal deep learning framework that combines:
\begin{itemize}
    \item Vision Transformer (ViT) with masked autoencoder (MAE) pretraining for 3D MRI feature extraction
    \item Feature Tokenizer Transformer (FT-Transformer) for tabular clinical data
    \item Cross-attention fusion for learned modality interaction
\end{itemize}

Our contributions are: (1) an end-to-end multimodal framework combining 3D ViT and FT-Transformer for AD classification; (2) validation on 6,065 subjects from three independent cohorts (ADNI, OASIS, NACC) with strict subject-level splitting; (3) cross-task generalization: training on the AD trajectory (including MCI converters) achieves 93.3\% on established AD detection; and (4) comprehensive ablation studies demonstrating the value of multimodal fusion.

\section{Related Work}

\subsection{Deep Learning for AD Classification}

Convolutional neural networks (CNNs) have dominated AD classification from MRI. Wen et al. \cite{wen2020} provided a comprehensive review, showing that 3D CNNs achieve up to 89\% balanced accuracy with proper methodology. More recently, Vision Transformers (ViT) \cite{dosovitskiy2021} have been adapted for 3D medical imaging. VECNN \cite{vecnn2024} combined ViT with CNNs, reaching 94.1\% on a single cohort.

\subsection{Methodological Concerns}

A critical issue in AD classification is data leakage, where test subjects' data inadvertently influences training. Yagis et al. \cite{yagis2021} quantified this effect: improper data splitting inflates accuracy by 25-55\%, with models achieving 96\% accuracy even on randomly assigned labels through memorization of individual brain morphology. Young et al. \cite{young2025} found that fewer than 20\% of published studies employ external validation, with studies lacking proper subject-level splitting reporting 95-99\% accuracy while rigorous studies achieve 66-90\%.

\subsection{Transformer Architectures}

Vision Transformers (ViT) \cite{dosovitskiy2021} divide images into patches, treating each as a token for self-attention. Unlike CNNs with local receptive fields, ViT captures global relationships---critical for detecting distributed atrophy patterns in AD. Masked autoencoder (MAE) pretraining \cite{he2022} enables ViT to learn robust representations from unlabeled data by reconstructing masked patches.

For tabular data, the FT-Transformer \cite{gorishniy2021} embeds each feature as a separate token, enabling attention across features. This demonstrates strong performance compared to traditional MLPs across diverse tabular benchmarks and narrows the performance gap with gradient boosting methods, though no universally superior solution emerges.

\subsection{Multimodal Fusion}

Combining imaging with clinical data improves classification performance. Qiu et al. \cite{qiu2020} demonstrated this with an end-to-end FCN-MLP framework that achieved 96.8\% accuracy on ADNI by integrating MRI features with age, sex, and MMSE scores. However, diagnostic scores like MMSE (median: 29 for CN vs 23.5 for AD) are strongly predictive of cognitive status, raising concerns about whether such models learn disease patterns versus cognitive assessment correlations. In a larger follow-up, Qiu et al. \cite{qiu2022} scaled to 8,916 subjects across 8 cohorts using a hybrid architecture that concatenates CNN-derived imaging features with CatBoost for tabular data fusion.

We advance this line of work by combining state-of-the-art architectures for both modalities: 3D Vision Transformer with masked autoencoder pretraining for imaging and Feature Tokenizer Transformer for clinical data, integrated through bidirectional cross-attention. We exclude diagnostic scores (MMSE, CDR) to ensure the model learns from structural neuroimaging and non-cognitive clinical features rather than test-score proxies of the diagnostic label.

\section{Methods}

\subsection{Datasets}

We aggregated data from three publicly available cohorts:

\textbf{ADNI (Alzheimer's Disease Neuroimaging Initiative):} Data used in the preparation of this article were obtained from the ADNI database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. We included 903 subjects (424 CN, 479 AD-trajectory).

\textbf{OASIS-3 (Open Access Series of Imaging Studies) \cite{oasis3}:} A longitudinal neuroimaging dataset from Washington University. We included 1,030 subjects (742 CN, 288 AD-trajectory).

\textbf{NACC (National Alzheimer's Coordinating Center):} The largest US Alzheimer's database, aggregating clinical data from over 40 research centers. MRI data were obtained from SCAN (Standardized Centralized Alzheimer's \& Related Dementias Neuroimaging), NACC's neuroimaging repository. We included 4,132 subjects (3,581 CN, 551 AD-trajectory).

The AD-trajectory group includes both clinically diagnosed AD patients and MCI subjects who subsequently progressed to AD, representing individuals on the Alzheimer's disease continuum. The combined dataset comprises 6,065 subjects (4,747 CN, 1,318 AD-trajectory; 78.3\% vs 21.7\%). Table~\ref{tab:dataset} summarizes the dataset characteristics.

\begin{table}[htbp]
\caption{Dataset Composition}
\begin{center}
\begin{tabular}{lccccc}
\toprule
\textbf{Cohort} & \textbf{Total} & \textbf{CN} & \textbf{AD-traj} & \textbf{\% Total} \\
\midrule
ADNI & 903 & 424 & 479 & 14.9\% \\
OASIS & 1,030 & 742 & 288 & 17.0\% \\
NACC & 4,132 & 3,581 & 551 & 68.1\% \\
\midrule
\textbf{Total} & \textbf{6,065} & \textbf{4,747} & \textbf{1,318} & 100\% \\
\bottomrule
\end{tabular}
\label{tab:dataset}
\end{center}
\end{table}

\subsection{Data Splitting Strategy}

To prevent data leakage, we employed strict subject-level splitting with 5-fold stratified cross-validation:
\begin{itemize}
    \item Each subject appears in exactly one split (train/val/test)
    \item Only the first visit per subject is used (no longitudinal leakage)
    \item All 6,065 subjects are pooled and split via stratified 5-fold CV
    \item Each fold: 80\% train+val / 20\% test (rotating test sets)
    \item Within each fold: train is split 90/10 into train/val
    \item Final splits per fold: 72\% train / 8\% val / 20\% test (4,367 / 485 / 1,213 subjects)
    \item Experiments repeated with 3 random seeds (42, 123, 456)
\end{itemize}

This full cross-validation approach ensures every subject serves as a test sample exactly once per seed, providing robust performance estimates. The methodology follows recommendations from Wen et al.\ \cite{wen2020} and addresses concerns raised by recent systematic reviews \cite{yagis2021}.

\subsection{MRI Preprocessing}

T1-weighted MRI scans underwent standardized preprocessing (Fig.~\ref{fig:preprocessing}):
\begin{enumerate}
    \item Conversion from DICOM to NIfTI format
    \item N4 bias field correction to remove intensity inhomogeneities
    \item Affine registration to MNI-152 template (1.75mm isotropic)
    \item Skull stripping using SynthStrip \cite{synthstrip}
    \item Resampling to 128$\times$128$\times$128 voxels
\end{enumerate}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/preprocessing_pipeline.png}}
\caption{MRI preprocessing pipeline showing (left to right): original T1-weighted scan, N4 bias-corrected image, MNI-152 registered volume, and skull-stripped brain.}
\label{fig:preprocessing}
\end{figure}

\subsection{Clinical Features}

We extracted 16 clinical features common across all three cohorts:
\begin{itemize}
    \item \textbf{Demographics (4):} Age, sex, education years, marital status
    \item \textbf{Neuropsychological tests (6):} Category fluency (animals), Trail Making Tests A/B, digit span forward/backward, Boston Naming Test
    \item \textbf{Physical measurements (2):} Weight, BMI
    \item \textbf{Medical history (4):} Alcohol use, smoking history, cardiovascular disease, neurological conditions
\end{itemize}

Missing values were imputed using median imputation and all features were z-score normalized. Importantly, diagnostic criteria scores (MMSE, CDR) were excluded to prevent data leakage.

\subsection{Model Architecture}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/architecture.pdf}}
\caption{Multimodal fusion architecture combining Vision Transformer (ViT) for 3D MRI and FT-Transformer for clinical features with bidirectional cross-attention fusion.}
\label{fig:architecture}
\end{figure}

Our multimodal fusion architecture (Fig.~\ref{fig:architecture}) consists of three components:

\textbf{MRI Encoder:} We use a Vision Transformer (ViT-Base) adapted for 3D volumes, pretrained using masked autoencoder (MAE) reconstruction with 75\% masking ratio. Input volumes are divided into 16$\times$16$\times$16 patches, each embedded into 768-dimensional tokens. The encoder consists of 12 transformer blocks.

\textbf{Tabular Encoder:} We employ the FT-Transformer \cite{gorishniy2021}, which embeds each feature independently before processing through transformer layers. We use 3 transformer blocks with 64-dimensional embeddings and 4 attention heads, producing a 64-dimensional output.

\textbf{Cross-Modal Fusion:} Both modality representations are fused using bidirectional cross-attention with 8 heads and hidden dimension 512:
\begin{align}
f'_{mri} &= \text{CrossAttn}(Q{=}f_{mri}, K{=}f_{tab}, V{=}f_{tab}) \\
f'_{tab} &= \text{CrossAttn}(Q{=}f_{tab}, K{=}f_{mri}, V{=}f_{mri}) \\
f_{fused} &= \text{MLP}([f'_{mri}; f'_{tab}])
\end{align}
This enables each modality to attend to complementary information from the other. Auxiliary classifiers on each branch (weighted 0.3) encourage modality-specific learning.

\subsection{Training Details}

Models were trained for 100 epochs with early stopping (patience=20, min\_epochs=40), saving the best checkpoint based on validation accuracy. Hyperparameters were selected based on preliminary experiments on a held-out validation set:
\begin{itemize}
    \item AdamW optimizer with weight decay 0.01
    \item Learning rate: 2$\times$10$^{-5}$ with cosine annealing
    \item 5 warmup epochs
    \item Layer-wise learning rate decay (0.75) for ViT fine-tuning
    \item Weighted cross-entropy loss with label smoothing (0.1)
    \item Batch size: 4 (due to 3D volume memory requirements)
    \item Test-time augmentation (8 augmentations)
\end{itemize}

Experiments used 5-fold cross-validation repeated with 3 random seeds (42, 123, 456) to assess stability, yielding 15 total training runs per model configuration.

\section{Results}

\subsection{The Alzheimer's Disease Continuum}

Our classification groups established AD patients with MCI subjects who subsequently progressed to AD (MCI converters). This grouping is grounded in the 2018 NIA-AA Research Framework \cite{jack2018}, which defines AD as a biological continuum rather than a clinical syndrome---individuals with AD pathology are on the ``Alzheimer's continuum'' regardless of clinical stage.

Longitudinal neuroimaging studies provide strong evidence that MCI converters exhibit AD-like atrophy patterns detectable on structural MRI. Misra et al. \cite{misra2009} demonstrated using ADNI data that MCI converters show significantly higher hippocampal atrophy rates ($-3.5\%$/year) compared to stable MCI patients ($-2.2\%$/year), with baseline MRI achieving 81.5\% accuracy in predicting future conversion. Importantly, converters had already reached ``levels of widespread and significant brain atrophy at baseline,'' indicating that structural changes precede clinical diagnosis.

\subsection{Classification Performance}

All models were trained on the \textbf{CN vs AD-trajectory} task, where AD-trajectory includes both clinically diagnosed AD patients and MCI subjects who later progressed to AD. We evaluate on two test sets: (1) an in-domain AD-trajectory test set matching the training distribution, and (2) a cross-task \textbf{CN vs established AD} test set containing only established AD patients without MCI converters.

Table~\ref{tab:results} presents our main results. On the in-domain AD-trajectory task, our multimodal model achieved 92.4\% accuracy with AUC of 0.959.

The same trajectory-trained model achieved \textbf{93.3\% accuracy} (AUC: 0.956) on the established AD test set. This higher cross-task performance demonstrates that training on heterogeneous presentations---including subtle early-stage MCI converters---produces robust features that generalize effectively to established disease detection, where atrophy patterns are more pronounced.

\begin{table}[htbp]
\caption{Classification Results: Cross-Task Generalization}
\begin{center}
\begin{tabular}{llcc}
\toprule
\textbf{Training Task} & \textbf{Test Set} & \textbf{Acc} & \textbf{AUC} \\
\midrule
\multicolumn{4}{@{}l}{\textit{Trained on CN vs AD-trajectory:}} \\
AD-trajectory & AD-trajectory (in-domain) & 92.4$\pm$1.0\% & 0.959$\pm$.01 \\
AD-trajectory & Established AD (cross-task) & \textbf{93.3$\pm$0.9\%} & 0.956$\pm$.01 \\
\midrule
\multicolumn{4}{@{}l}{\textit{Trained on CN vs Established AD:}} \\
Established AD & Established AD (in-domain) & 92.9$\pm$0.7\% & 0.948$\pm$.01 \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{center}
\end{table}

\subsection{Ablation Study}

To quantify the contribution of each modality, we evaluated single-modality baselines using identical training protocols on the AD-trajectory task (Table~\ref{tab:ablation}). All models used 5-fold cross-validation with 3 random seeds. The MRI-only model (ViT with MAE pretraining) achieved 83.3\% accuracy with AUC of 0.826. The tabular-only model (FT-Transformer) achieved 84.9\% accuracy with AUC of 0.925.

The multimodal fusion achieves substantial gains over both unimodal baselines: \textbf{+9.1\%} accuracy over MRI-only and \textbf{+7.5\%} over tabular-only. This confirms that the modalities provide complementary information that cross-attention fusion effectively captures. Notably, tabular features alone achieve strong performance, suggesting that cognitive assessments and clinical variables carry significant diagnostic value.

\begin{table}[htbp]
\caption{Ablation Study: Modality Contributions (AD-trajectory task)}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Acc.} & \textbf{AUC} \\
\midrule
MRI only (ViT) & 83.3$\pm$0.7\% & 0.826 \\
Tabular only (FT-T) & 84.9$\pm$0.4\% & 0.925 \\
\midrule
\textbf{Multimodal (Ours)} & \textbf{92.4$\pm$1.0\%} & \textbf{0.959} \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{center}
\vspace{-2mm}
{\scriptsize Note: Results on AD-trajectory test set with 5-fold CV. Cross-task evaluation on established AD achieves 93.3\% (Table~\ref{tab:results}).}
\end{table}

\subsection{Cross-Cohort Generalization}

To demonstrate the importance of multi-cohort training, we conducted an additional experiment training exclusively on ADNI data (903 subjects) and evaluating on held-out ADNI subjects (in-domain) as well as OASIS (1,030 subjects) and NACC (4,132 subjects) as external validation sets (Table~\ref{tab:external}).

\begin{table}[htbp]
\caption{Single-Cohort vs Multi-Cohort Training}
\begin{center}
\begin{tabular}{llcc}
\toprule
\textbf{Training} & \textbf{Test Set} & \textbf{Acc} & \textbf{AUC} \\
\midrule
\multicolumn{4}{@{}l}{\textit{ADNI-only training (N=903):}} \\
ADNI-only & ADNI (in-domain) & 81.4$\pm$3.5\% & 0.90$\pm$.02 \\
ADNI-only & OASIS (external) & 56.5$\pm$9.2\% & 0.64$\pm$.05 \\
ADNI-only & NACC (external) & 38.5$\pm$13.9\% & 0.89$\pm$.05 \\
\midrule
\multicolumn{4}{@{}l}{\textit{Multi-cohort training (N=6,065):}} \\
Multi-cohort & All cohorts & \textbf{92.4$\pm$1.0\%} & \textbf{0.959$\pm$.01} \\
\bottomrule
\end{tabular}
\label{tab:external}
\end{center}
\vspace{-2mm}
{\scriptsize Note: All results from 5-fold CV with 3 seeds (15 runs total).}
\end{table}

The results reveal a significant \textbf{cross-cohort generalization gap}: while single-cohort training achieves 81.4$\pm$3.5\% on ADNI, accuracy drops to 56.5$\pm$9.2\% on OASIS and 38.5$\pm$13.9\% on NACC. The high variance on external cohorts reflects inconsistent generalization across folds. Notably, NACC shows high AUC (0.89) despite low accuracy---this discrepancy arises from class distribution shift: ADNI has 47\% CN while NACC has 87\% CN, causing the model's decision threshold to be miscalibrated, resulting in high sensitivity (98\%) but poor specificity (29\%). This performance degradation demonstrates that models trained on a single cohort learn cohort-specific patterns that fail to generalize. In contrast, our multi-cohort approach achieves 92.4\% overall accuracy by learning cohort-invariant features, validating the importance of diverse training data for robust AD classification.

\subsection{Comparison with Literature}

Table~\ref{tab:comparison} contextualizes our results within the broader literature. Studies with proper subject-level splitting report accuracies in the 66-90\% range \cite{young2025}, while those with potential data leakage claim 95-99\%.

\begin{table}[htbp]
\caption{Comparison with State-of-the-Art Methods}
\begin{center}
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}lcccrr@{}}
\toprule
\textbf{Study} & \textbf{Data} & \textbf{Coh.} & \textbf{N} & \textbf{Acc} & \textbf{AUC} \\
\midrule
\multicolumn{6}{@{}l}{\textit{Proper subject-level splitting:}} \\
\textbf{Ours (established)} & MRI+Tab & 3 & 6,065 & \textbf{93.3\%} & \textbf{0.96} \\
\textbf{Ours (traj)} & MRI+Tab & 3 & 6,065 & 92.4\% & 0.96 \\
Qiu \cite{qiu2022} & MRI+Tab & 8 & 8,916 & ---$^*$ & .94--.97$^*$ \\
VECNN \cite{vecnn2024} & MRI & 1 & 818 & 94.1\% & --- \\
Wen \cite{wen2020} & MRI & 3 & 1k & 89\%$^\dagger$ & --- \\
\midrule
\multicolumn{6}{@{}l}{\textit{Potential data leakage:}} \\
Various \cite{young2025} & MRI & 1 & $<$1k & 95-99\% & --- \\
\bottomrule
\end{tabular}
\label{tab:comparison}
\end{center}
\vspace{-2mm}
{\scriptsize $^\dagger$Bal. acc.; $^*$Binary subtasks, not comparable}
\end{table}

Our 93.3\% accuracy on established AD detection and 92.4\% on the challenging AD-trajectory task significantly exceed the 66-90\% range reported by Young et al. \cite{young2025} for methodologically rigorous studies. Key differentiators of our work:

\textbf{Scale and diversity:} With 6,065 subjects from 3 cohorts, ours is among the largest multi-cohort studies with proper methodology. Most ViT-based approaches use single cohorts with $<$1,000 subjects.

\textbf{End-to-end multimodal learning:} Unlike Qiu et al. \cite{qiu2022} who use separate CNN and CatBoost models, our approach learns joint MRI-clinical representations through cross-attention fusion.

\textbf{ViT improvement:} Compared to single-modality ViT approaches like VECNN \cite{vecnn2024} (94.1\% on single cohort), our multimodal fusion achieves comparable performance (93.3\% on established AD) while generalizing across three cohorts.

\section{Discussion}

\subsection{Cross-Task Generalization}

Our results show that models trained on the heterogeneous AD-trajectory task (including MCI converters) achieve strong performance on established AD detection (93.3\%), only slightly below the in-domain trajectory performance (92.4\%). This demonstrates robust generalization: MCI converters exhibit subtle, early-stage atrophy patterns that are harder to distinguish from normal aging. Training on these challenging cases forces the model to learn fine-grained discriminative features---subtle hippocampal volume differences, distributed cortical thinning patterns, and nuanced feature interactions between imaging and clinical data.

When evaluated on established AD cases with more pronounced atrophy, the learned features transfer effectively. The small performance gap suggests that features learned from heterogeneous presentations generalize well to established disease detection, supporting the clinical utility of trajectory-based training for robust AD classification.

\subsection{Limitations}

Several limitations should be acknowledged:

\textbf{Class imbalance:} The 78\%/22\% CN/AD ratio affects sensitivity. Future work could explore oversampling or focal loss.

\textbf{Missing data:} Some tabular features have incomplete coverage across cohorts (23-100\%), handled via median imputation. Cohort-specific models could leverage richer feature sets.

\textbf{Single-cohort limitations:} Our cross-cohort experiment (Table~\ref{tab:external}) demonstrates significant performance drops when training on ADNI alone and testing on external cohorts, highlighting the importance of diverse training data.

\subsection{Future Directions}

Several extensions merit investigation. First, incorporating attention visualization could identify which brain regions and clinical features drive predictions, improving interpretability for clinical adoption. Second, extending to multi-class classification (CN vs MCI vs AD) would enable earlier intervention. Finally, prospective clinical validation is needed before deployment as a screening tool.

\section{Conclusion}

We presented a multimodal deep learning framework for Alzheimer's disease classification combining Vision Transformer and Feature Tokenizer Transformer with cross-attention fusion. Validated on 6,065 subjects from three cohorts with strict subject-level splitting, we achieve 92.4\% accuracy on the challenging AD-trajectory task and 93.3\% on established AD detection through cross-task generalization. Our cross-cohort experiments demonstrate that single-cohort training fails to generalize to external datasets, validating the importance of multi-cohort training for robust clinical deployment. These results exceed the 66-90\% range reported by rigorous studies while maintaining methodological rigor.

\section*{Acknowledgment}

*Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in the analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how\_to\_apply/ADNI\_Acknowledgement\_List.pdf

Data collection and sharing for the Alzheimer's Disease Neuroimaging Initiative (ADNI) is funded by the National Institute on Aging (National Institutes of Health Grant U19AG024904). The grantee organization is the Northern California Institute for Research and Education. In the past, ADNI has also received funding from the National Institute of Biomedical Imaging and Bioengineering, the Canadian Institutes of Health Research, and private sector contributions through the Foundation for the National Institutes of Health (FNIH) including generous contributions from the following: AbbVie, Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research \& Development, LLC.; Johnson \& Johnson Pharmaceutical Research \& Development LLC.; Lumosity; Lundbeck; Merck \& Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics.

Data were provided in part by OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P30 AG066444, P50 AG00561, P01 AG026276, P01 AG003991.

The NACC database is funded by NIA/NIH Grant U24 AG072122. SCAN is a multi-institutional project that was funded as a U24 grant (AG067418) by the National Institute on Aging in May 2020. Data collected by SCAN and shared by NACC are contributed by the NIA-funded ADRCs as follows: Arizona Alzheimer's Center - P30 AG072980 (PI: Eric Reiman, MD); R01 AG069453 (PI: Eric Reiman (contact), MD); P30 AG019610 (PI: Eric Reiman, MD); and the State of Arizona which provided additional funding supporting our center; Boston University - P30 AG013846 (PI Neil Kowall MD); Cleveland ADRC - P30 AG062428 (James Leverenz, MD); Cleveland Clinic, Las Vegas - P20AG068053; Columbia - P50 AG008702 (PI Scott Small MD); Duke/UNC ADRC - P30 AG072958; Emory University - P30AG066511 (PI Levey Allan, MD, PhD); Indiana University - R01 AG19771 (PI Andrew Saykin, PsyD); P30 AG10133 (PI Andrew Saykin, PsyD); P30 AG072976 (PI Andrew Saykin, PsyD); R01 AG061788 (PI Shannon Risacher, PhD); R01 AG053993 (PI Yu-Chien Wu, MD, PhD); U01 AG057195 (PI Liana Apostolova, MD); U19 AG063911 (PI Bradley Boeve, MD); and the Indiana University Department of Radiology and Imaging Sciences; Johns Hopkins - P30 AG066507 (PI Marilyn Albert, Phd.); Mayo Clinic - P50 AG016574 (PI Ronald Petersen MD PhD); Mount Sinai - P30 AG066514 (PI Mary Sano, PhD); R01 AG054110 (PI Trey Hedden, PhD); R01 AG053509 (PI Trey Hedden, PhD); New York University - P30AG066512-01S2 (PI Thomas Wisniewski, MD); R01AG056031 (PI Ricardo Osorio, MD); R01AG056531 (PIs Ricardo Osorio, MD; Girardin Jean-Louis, PhD); Northwestern University - P30 AG013854 (PI Robert Vassar PhD); R01 AG045571 (PI Emily Rogalski, PhD); R56 AG045571, (PI Emily Rogalski, PhD); R01 AG067781, (PI Emily Rogalski, PhD); U19 AG073153, (PI Emily Rogalski, PhD); R01 DC008552, (M.-Marsel Mesulam, MD); R01 AG077444, (PIs M.-Marsel Mesulam, MD, Emily Rogalski, PhD); R01 NS075075 (PI Emily Rogalski, PhD); R01 AG056258 (PI Emily Rogalski, PhD); Oregon Health and Science University - P30 AG008017 (PI Jeffrey Kaye MD); R56 AG074321 (PI Jeffrey Kaye, MD); Rush University - P30 AG010161 (PI David Bennett MD); Stanford - P30AG066515; P50 AG047366 (PI Victor Henderson MD MS); University of Alabama, Birmingham - P20; University of California, Davis - P30 AG10129 (PI Charles DeCarli, MD); P30 AG072972 (PI Charles DeCarli, MD); University of California, Irvine - P50 AG016573 (PI Frank LaFerla PhD); University of California, San Diego - P30AG062429 (PI James Brewer, MD, PhD); University of California, San Francisco - P30 AG062422 (Rabinovici, Gil D., MD); University of Kansas - P30 AG035982 (Russell Swerdlow, MD); University of Kentucky - P30 AG028283-15S1 (PIs Linda Van Eldik, PhD and Brian Gold, PhD); University of Michigan ADRC - P30AG053760 (PI Henry Paulson, MD, PhD) P30AG072931 (PI Henry Paulson, MD, PhD) Cure Alzheimer's Fund 200775 - (PI Henry Paulson, MD, PhD) U19 NS120384 (PI Charles DeCarli, MD, University of Michigan Site PI Henry Paulson, MD, PhD) R01 AG068338 (MPI Bruno Giordani, PhD, Carol Persad, PhD, Yi Murphey, PhD) S10OD026738-01 (PI Douglas Noll, PhD) R01 AG058724 (PI Benjamin Hampstead, PhD) R35 AG072262 (PI Benjamin Hampstead, PhD) W81XWH2110743 (PI Benjamin Hampstead, PhD) R01 AG073235 (PI Nancy Chiaravalloti, University of Michigan Site PI Benjamin Hampstead, PhD) 1I01RX001534 (PI Benjamin Hampstead, PhD) IRX001381 (PI Benjamin Hampstead, PhD); University of New Mexico - P20 AG068077 (Gary Rosenberg, MD); University of Pennsylvania - State of PA project 2019NF4100087335 (PI David Wolk, MD); Rooney Family Research Fund (PI David Wolk, MD); R01 AG055005 (PI David Wolk, MD); University of Pittsburgh - P50 AG005133 (PI Oscar Lopez MD); University of Southern California - P50 AG005142 (PI Helena Chui MD); University of Washington - P50 AG005136 (PI Thomas Grabowski MD); University of Wisconsin - P50 AG033514 (PI Sanjay Asthana MD FRCP); Vanderbilt University - P20 AG068082; Wake Forest - P30AG072947 (PI Suzanne Craft, PhD); Washington University, St. Louis - P01 AG03991 (PI John Morris MD); P01 AG026276 (PI John Morris MD); P20 MH071616 (PI Dan Marcus); P30 AG066444 (PI John Morris MD); P30 NS098577 (PI Dan Marcus); R01 AG021910 (PI Randy Buckner); R01 AG043434 (PI Catherine Roe); R01 EB009352 (PI Dan Marcus); UL1 TR000448 (PI Brad Evanoff); U24 RR021382 (PI Bruce Rosen); Avid Radiopharmaceuticals / Eli Lilly; Yale - P50 AG047270 (PI Stephen Strittmatter MD PhD); R01AG052560 (MPI: Christopher van Dyck, MD; Richard Carson, PhD); R01AG062276 (PI: Christopher van Dyck, MD); 1Florida - P30AG066506-03 (PI Glenn Smith, PhD); P50 AG047266 (PI Todd Golde MD PhD).

\begin{thebibliography}{00}
\bibitem{who2023} World Health Organization, ``Dementia,'' 2023. [Online]. Available: https://www.who.int/health-topics/dementia

\bibitem{young2025} V. M. Young, S. Gates, L. Y. Garcia, and A. Salardini, ``Data leakage in deep learning for Alzheimer's disease diagnosis: A scoping review of methodological rigor and performance inflation,'' \textit{Diagnostics}, vol. 15, no. 18, p. 2348, 2025.

\bibitem{yagis2021} E. Yagis, A. G. S. De Herrera, and L. Citi, ``Effect of data leakage in brain MRI classification using 2D convolutional neural networks,'' \textit{Sci. Rep.}, vol. 11, no. 22544, 2021.

\bibitem{qiu2020} S. Qiu et al., ``Development and validation of an interpretable deep learning framework for Alzheimer's disease classification,'' \textit{Brain}, vol. 143, no. 6, pp. 1920--1933, 2020.

\bibitem{wen2020} J. Wen et al., ``Convolutional neural networks for classification of Alzheimer's disease: Overview and reproducible evaluation,'' \textit{Med. Image Anal.}, vol. 63, p. 101694, 2020.

\bibitem{dosovitskiy2021} A. Dosovitskiy et al., ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \textit{Proc. ICLR}, 2021.

\bibitem{vecnn2024} Z. Zhao et al., ``Vision transformer-equipped convolutional neural networks for automated Alzheimer's disease diagnosis using 3D MRI scans,'' \textit{Front. Neurol.}, vol. 15, p. 1490829, 2024.

\bibitem{he2022} K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, ``Masked autoencoders are scalable vision learners,'' in \textit{Proc. CVPR}, 2022.

\bibitem{gorishniy2021} Y. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko, ``Revisiting deep learning models for tabular data,'' in \textit{Proc. NeurIPS}, 2021.

\bibitem{qiu2022} S. Qiu et al., ``Multimodal deep learning for Alzheimer's disease dementia assessment,'' \textit{Nat. Commun.}, vol. 13, p. 3404, 2022.

\bibitem{synthstrip} A. Hoopes, J. S. Mora, A. V. Dalca, B. Fischl, and M. Hoffmann, ``SynthStrip: Skull-stripping for any brain image,'' \textit{NeuroImage}, vol. 260, p. 119474, 2022.

\bibitem{jack2018} C. R. Jack Jr. et al., ``NIA-AA Research Framework: Toward a biological definition of Alzheimer's disease,'' \textit{Alzheimer's Dement.}, vol. 14, no. 4, pp. 535--562, 2018.

\bibitem{misra2009} C. Misra, Y. Fan, and C. Davatzikos, ``Baseline and longitudinal patterns of brain atrophy in MCI patients, and their use in prediction of short-term conversion to AD: Results from ADNI,'' \textit{NeuroImage}, vol. 44, no. 4, pp. 1415--1422, 2009.

\bibitem{oasis3} P. J. LaMontagne et al., ``OASIS-3: Longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and Alzheimer disease,'' \textit{medRxiv}, 2019. doi: 10.1101/2019.12.13.19014902.

\end{thebibliography}

\end{document}
