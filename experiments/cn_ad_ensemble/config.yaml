# Configuration for CN vs AD Binary Classification with SEResNet-18 Ensemble
#
# This experiment uses the IMBALMED-style balanced ensemble approach
# with SEResNet-18 (ResNet-18 + Squeeze-and-Excitation blocks)
#
# Architecture: SEResNet-18 with channel attention
# Paper: "Deep CNN ResNet-18 based model with attention and transfer learning
#         for Alzheimer's disease detection" (2024) - achieved 93.26% on ADNI
#
# Usage:
#   Step 1: python 01_create_cn_ad_splits.py --config config.yaml
#   Step 2: python 02_create_balanced_subsets.py --config config.yaml
#   Step 3: python train_ensemble.py --config config.yaml
#   Step 4: python predict_ensemble.py --config config.yaml

# =============================================================================
# DATA PATHS - Edit these for your setup
# =============================================================================
data:
  # Input data (preprocessed MRI scans)
  # Option 1: ADNI-skull (ANTs preprocessing - recommended)
  skull_dir: "/path/to/ADNI_skull"  # UPDATE THIS: Path to ADNI-skull dataset

  # Option 2: NPPY preprocessing (alternative)
  # skull_dir: "/path/to/cn_mci_ad"  # UPDATE THIS: Path to NPPY dataset

  # Source 3-class splits (to be filtered to binary CN/AD)
  source_3class_train: "../cn_mci_ad_medicalnet/data/splits/train.csv"
  source_3class_val: "../cn_mci_ad_medicalnet/data/splits/val.csv"
  source_3class_test: "../cn_mci_ad_medicalnet/data/splits/test.csv"

  # Output paths (relative to this experiment folder)
  splits_dir: "data/splits"                        # Binary CN/AD splits
  balanced_subsets_dir: "balanced_subsets_3models" # Balanced training subsets
  checkpoints_dir: "checkpoints_seresnet_3models"  # Model checkpoints
  results_dir: "results"                           # Evaluation results
  logs_dir: "logs"                                 # Training logs

  # Automatically generated paths (don't edit)
  train_csv: "data/splits/train.csv"
  val_csv: "data/splits/val.csv"
  test_csv: "data/splits/test.csv"

# =============================================================================
# DATASET PREPARATION (Step 1 & 2)
# =============================================================================
dataset_preparation:
  # Binary split creation (filter out MCI from 3-class data)
  filter_mci: true                # Remove MCI samples
  relabel_ad: true                # Relabel AD from 2 to 1 for binary

  # Expected distribution after filtering:
  # Train: ~867 samples (CN=590, AD=277)
  # Val:   ~109 samples (CN=74, AD=35)
  # Test:  ~109 samples (CN=74, AD=35)

  # Balanced subset creation (IMBALMED approach)
  num_subsets: 3                  # Number of balanced subsets for ensemble
  balance_method: "downsample"    # Downsample majority class to minority

  # Each balanced subset will have:
  # CN=277, AD=277 (total 554 samples per subset)

  # Reproducibility
  random_seed: 42                 # Random seed for splitting

# =============================================================================
# MODEL ARCHITECTURE - SEResNet-18
# =============================================================================
model:
  name: "SEResNet-18"

  # Image/volume configuration
  image_size: 192                 # Input volume size (192x192x192)
  num_channels: 1                 # Number of input channels (grayscale MRI)
  num_classes: 2                  # Binary classification (CN vs AD)

  # Architecture details
  architecture: "seresnet18"      # Model variant: resnet18 or seresnet18
  use_se_blocks: true             # Enable Squeeze-and-Excitation blocks
  se_reduction: 16                # SE reduction ratio (channels / reduction)

  # SE Block details:
  # - Adds channel attention with minimal overhead (+87K params, +0.3%)
  # - Squeeze: Global average pooling (spatial → channel descriptor)
  # - Excitation: FC(C→C/16) → ReLU → FC(C/16→C) → Sigmoid
  # - Scale: Multiply features by learned channel weights

  # Pretrained weights (MedicalNet)
  pretrained_path: "../cn_mci_ad_medicalnet/pretrained/resnet_18_23dataset.pth"
  use_pretrained: true            # Load pretrained weights
  freeze_backbone: false          # Don't freeze backbone (fine-tune all layers)

# =============================================================================
# ENSEMBLE CONFIGURATION (IMBALMED-inspired)
# =============================================================================
ensemble:
  num_models: 3                   # Number of models in ensemble

  # Balanced subset strategy
  balance_method: "downsample"    # How to balance classes
  # Creates num_models balanced subsets
  # Each subset: CN=277, AD=277 (equal to minority class size)
  # Different CN samples in each subset for diversity

  # Training strategy
  train_individual: true          # Train each model separately
  save_individual: true           # Save each model checkpoint

  # Prediction strategy
  aggregation: "average"          # How to combine predictions (average probabilities)
  save_individual_preds: true     # Save predictions from each model

# =============================================================================
# TRAINING HYPERPARAMETERS
# =============================================================================
training:
  batch_size: 4                   # Batch size (adjust for GPU memory)
  epochs: 50                      # Maximum number of epochs
  learning_rate: 0.0001           # Initial learning rate (1e-4)
  weight_decay: 0.0001            # Weight decay for AdamW (L2 regularization)
  optimizer: "AdamW"              # Optimizer: AdamW

  # Learning rate scheduler
  scheduler: "CosineAnnealingLR"  # Cosine annealing with warm restarts
  min_lr: 1e-6                    # Minimum learning rate
  T_max: 50                       # Cosine annealing period (same as epochs)

  # Class balancing
  use_weighted_loss: false        # Don't use weighted loss (already balanced via subsets)
  use_weighted_sampling: false    # Don't use weighted sampling (already balanced)

  # Reproducibility
  seed: 42                        # Random seed

# =============================================================================
# DATA AUGMENTATION
# =============================================================================
augmentation:
  enabled: false                  # Disable for now (can enable later for +5-10% improvement)

  # When enabled, use these settings:
  random_flip_prob: 0.5           # Probability of random left-right flip
  random_rotation_degrees: 10     # Max rotation degrees (±10°)
  gaussian_noise_std: 0.01        # Gaussian noise std deviation
  intensity_scale_range: [0.9, 1.1]  # Random intensity scaling
  random_affine: false            # Apply random affine transforms

# =============================================================================
# HARDWARE & PERFORMANCE
# =============================================================================
hardware:
  device: "cuda"                  # cuda or cpu (auto-detect if cuda available)
  num_workers: 4                  # Number of dataloader workers
  pin_memory: true                # Pin memory for faster GPU transfer
  mixed_precision: false          # Use automatic mixed precision (FP16)

# =============================================================================
# CALLBACKS & MONITORING
# =============================================================================
callbacks:
  early_stopping:
    enabled: true
    patience: 10                  # Stop if no improvement after N epochs
    monitor: "val_accuracy"       # Metric to monitor
    mode: "max"                   # Maximize validation accuracy

  checkpoint:
    save_top_k: 1                 # Save only best checkpoint (save disk space)
    monitor: "val_accuracy"       # Metric to monitor for best checkpoint
    mode: "max"                   # Maximize
    save_last: false              # Don't save last checkpoint (only best)

  logging:
    log_every_n_steps: 10         # Log metrics every N steps
    save_training_plot: true      # Save training curves (loss/accuracy)

# =============================================================================
# WEIGHTS & BIASES (WANDB) LOGGING
# =============================================================================
wandb:
  enabled: false                  # Enable/disable wandb logging
  project: "alzheimer-research"
  entity: null                    # Wandb entity/team name (null = use default)
  run_name: "cn-ad-seresnet18-ensemble"  # Custom run name
  log_model: false                # Upload model checkpoints to wandb
  tags:
    - seresnet-18
    - squeeze-excitation
    - ensemble
    - imbalmed
    - binary-classification
    - cn-vs-ad
  notes: "SEResNet-18 ensemble for binary CN vs AD classification (IMBALMED approach)"

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  metrics:
    - accuracy
    - balanced_accuracy
    - precision
    - recall
    - f1
    - confusion_matrix

  save_predictions: true          # Save predictions on test set
  save_probabilities: true        # Save prediction probabilities

  # Target names for reports
  target_names: ["CN", "AD"]

# =============================================================================
# EXPECTED PERFORMANCE (Based on Literature and Experiments)
# =============================================================================
# Dataset Statistics (after filtering MCI):
#   Train: 867 samples (CN=590, AD=277)
#   Val:   109 samples (CN=74, AD=35)
#   Test:  109 samples (CN=74, AD=35)
#
# Balanced Subsets (3 models):
#   Each subset: 554 samples (CN=277, AD=277)
#   Total training data used: 1662 samples across 3 models
#
# Expected Results (3-model ensemble):
#   - Test Accuracy: 89-91%
#   - Balanced Accuracy: 86-88%
#   - Individual model val acc: 87-89%
#
# Achieved Results (5-model ensemble):
#   - Test Accuracy: 90.83%
#   - Balanced Accuracy: 87.22%
#   - CN Precision: 90.0%, Recall: 97.3%
#   - AD Precision: 93.1%, Recall: 77.1%
#
# Comparison with 3-Class:
#   - 3-class (CN/MCI/AD): 63.0% test accuracy (vanilla ResNet-18)
#   - Binary (CN/AD): 90.83% test accuracy (SEResNet-18)
#   - Improvement: +27.8% by removing hard-to-classify MCI
#
# Next Steps to Improve:
#   1. Enable data augmentation → Expected +5-10%
#   2. Increase ensemble size to 5-7 models → Expected +1-2%
#   3. Use whole brain (176x208x176) instead of hippocampus → Test impact
#   4. Implement focal loss for hard examples → Expected +2-3%
