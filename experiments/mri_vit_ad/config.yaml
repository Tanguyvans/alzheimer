# ViT for Alzheimer's Disease Classification
# Based on MICCAI 2024: "Training ViT with Limited Data for Alzheimer's"
# GitHub: https://github.com/qasymjomart/ViT_recipe_for_AD

experiment:
  name: "vit_base_cn_ad_trajectory"
  task: "cn_ad_trajectory"  # Binary: CN vs (MCIc + AD)
  description: "3D ViT-B with MAE pre-trained weights on MRI data"

model:
  architecture: "vit_base"  # Paper uses vit_base with MAE pretrained weights
  num_classes: 2  # Binary: CN vs AD_trajectory
  in_channels: 1
  image_size: 128  # 128x128x128 (paper uses this size, must be divisible by patch_size=16)
  dropout: 0.1  # Dropout in transformer blocks
  classifier_dropout: 0.5  # Paper uses 0.5 for classifier head
  drop_path_rate: 0.1  # Stochastic depth
  use_pretrained: true  # Use MAE pretrained weights
  pretrained_path: "pretrained/vit_mae75_pretrained.pth"

data:
  train_csv: "data/cn_ad_trajectory/train.csv"
  val_csv: "data/cn_ad_trajectory/val.csv"
  test_csv: "data/cn_ad_trajectory/test.csv"
  checkpoints_dir: "checkpoints"
  logs_dir: "logs"
  results_dir: "results"

preprocessing:
  use_paper_preprocessing: true  # Enable 1.75mm resampling + RAS orientation
  target_spacing: 1.75  # Paper uses 1.75mm isotropic voxel spacing

training:
  epochs: 100
  batch_size: 2  # Paper uses 1, but 2 for efficiency (128^3 uses more memory)
  learning_rate: 0.00001  # Paper: 1e-5
  weight_decay: 0.1  # Reduced from 0.3 for more stable training
  lr_min: 0.000001
  optimizer: "adamw"
  scheduler: "cosine"
  warmup_epochs: 10  # Increased warmup for more stable start
  freeze_backbone_epochs: 0  # Set to 0 when using layer-wise LR decay
  layer_wise_lr_decay: 0.75  # Paper: 0.75 decay factor per layer
  gradient_accumulation_steps: 4  # Effective batch size = 2 * 4 = 8
  gradient_clip: 0.5  # Reduced from 1.0 for stability
  use_weighted_loss: true
  label_smoothing: 0.0  # Disabled - can hurt with small datasets
  seed: 42

augmentation:
  enabled: true
  random_flip: true
  random_rotate: 15
  random_scale: [0.9, 1.1]
  random_intensity: 0.1

hardware:
  device: "cuda"  # mps for Mac M1/M2, cuda for GPU
  num_workers: 4
  pin_memory: true
  mixed_precision: false

callbacks:
  early_stopping:
    enabled: true
    patience: 20  # More patience for ViT
    monitor: "val_accuracy"
  checkpoint:
    save_best: true
    save_last: true

wandb:
  enabled: false
  project: "alzheimer-classification"
  entity: null
  run_name: "vit_base_mae"
  tags: ["vit", "transformer", "mae", "pretrained", "miccai2024"]
  notes: "3D ViT-B with MAE self-supervised pre-training"
