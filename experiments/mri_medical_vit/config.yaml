# 3D Vision Transformer Configuration
# For Alzheimer's Disease Classification (CN vs MCI vs AD)

experiment:
  name: "vit_small_3d"
  description: "3D Vision Transformer for brain MRI classification"

model:
  architecture: "vit_small"  # vit_tiny, vit_small, vit_base, vit_large
  num_classes: 3
  in_channels: 1
  image_size: 96  # 96x96x96 (must be divisible by patch_size)
  patch_size: 16  # 16x16x16 patches -> 216 patches
  dropout: 0.1
  pool: "cls"  # cls or mean
  use_pretrained: false
  pretrained_path: null  # Path to pretrained checkpoint
  model_type: "vit"  # vit, vit_classifier, or hybrid

data:
  train_csv: "data/train.csv"
  val_csv: "data/val.csv"
  test_csv: "data/test.csv"
  checkpoints_dir: "checkpoints"
  logs_dir: "logs"
  results_dir: "results"

training:
  epochs: 100
  batch_size: 4  # Small batch for 3D volumes
  learning_rate: 0.0001
  weight_decay: 0.05  # Higher for ViT
  lr_min: 0.000001
  warmup_epochs: 5
  optimizer: "adamw"
  scheduler: "cosine_warmup"
  use_weighted_loss: true
  label_smoothing: 0.1
  gradient_clip: 1.0
  seed: 42

augmentation:
  enabled: true
  random_flip: true
  random_rotate: 15
  random_scale: [0.9, 1.1]
  random_intensity: 0.1
  mixup_alpha: 0.0  # Set > 0 to enable mixup

hardware:
  device: "mps"  # mps for Mac M1/M2, cuda for GPU
  num_workers: 4
  pin_memory: true
  mixed_precision: false

callbacks:
  early_stopping:
    enabled: true
    patience: 20
    monitor: "val_accuracy"
  checkpoint:
    save_best: true
    save_last: true

wandb:
  enabled: false
  project: "alzheimer-classification"
  entity: null
  run_name: "vit_small_3d"
  tags: ["vit", "transformer", "3d", "mri"]
  notes: "3D Vision Transformer for Alzheimer's classification"
